{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gymnasium as gym\n",
    "import csv\n",
    "import time\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDiffusionNet(nn.Module):\n",
    "    def __init__(self,data_dim,cond_dim):\n",
    "        super(ConditionalDiffusionNet,self).__init__()\n",
    "        n_unit = 256\n",
    "\n",
    "        self.l1 = nn.Linear(data_dim, n_unit)\n",
    "        self.l2 = nn.Linear(n_unit, n_unit)\n",
    "\n",
    "        self.l1_beta = nn.Linear(1, n_unit)\n",
    "        self.l2_beta = nn.Linear(n_unit, n_unit)\n",
    "\n",
    "        self.l1_cond = nn.Linear(cond_dim, n_unit)\n",
    "        self.l2_cond = nn.Linear(n_unit, n_unit)\n",
    "\n",
    "        self.l3 = nn.Linear(n_unit,n_unit)\n",
    "        self.l4 = nn.Linear(n_unit,data_dim)\n",
    "    \n",
    "    def forward(self,x,c,t):\n",
    "        xx = self.l1(x)\n",
    "        xx = F.relu(xx)\n",
    "        xx = self.l2(xx)\n",
    "        xx = F.relu(xx)\n",
    "\n",
    "        cc = self.l1_cond(c)\n",
    "        cc = F.relu(cc)\n",
    "        cc = self.l2_cond(cc)\n",
    "        cc = F.relu(cc)\n",
    "\n",
    "        bb = self.l1_beta(t)\n",
    "        bb = F.relu(bb)\n",
    "        bb = self.l2_beta(bb)\n",
    "        bb = F.relu(bb)\n",
    "\n",
    "        xx = self.l3(xx+bb+cc)\n",
    "        xx = F.relu(xx)\n",
    "        xx = self.l4(xx)\n",
    "\n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDenoisingDiffusionProbabilisticModel():\n",
    "    def __init__(self, X, cond, beta, device, batch_size=32):\n",
    "        self.device = device\n",
    "\n",
    "        self.X = X\n",
    "        self.x_dim = self.X.shape[1]\n",
    "        self.C = cond\n",
    "        self.c_dim = self.C.shape[1]\n",
    "        self.beta = beta\n",
    "        self.n_beta = self.beta.shape[0]\n",
    "\n",
    "        alpha = 1 - self.beta\n",
    "        self.alpha = torch.tensor([[torch.prod(alpha[:i+1])] for i in range(self.n_beta)]).float()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = ConditionalDiffusionNet(self.X.shape[1], self.C.shape[1]).to(self.device)\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(self.X, self.C)\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "    def learning(self, n_epoch=10):\n",
    "        self.model.train()\n",
    "\n",
    "        for e in range(n_epoch):\n",
    "            for (x_batch, c_batch) in self.train_loader:\n",
    "                loss_hist = []\n",
    "\n",
    "                x_batch = x_batch\n",
    "                c_batch = c_batch\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                t = torch.randint(low=0, high=self.n_beta, size=(x_batch.shape[0],))\n",
    "                noise = torch.randn(x_batch.shape[0], self.x_dim)\n",
    "\n",
    "\n",
    "                x_t = torch.sqrt(self.alpha[t]) * x_batch + torch.sqrt(1-self.alpha[t]) * noise\n",
    "\n",
    "                noise_pred = self.model(x_t.to(self.device),\n",
    "                                        c_batch.to(self.device),\n",
    "                                        t[:,None].float().to(self.device))\n",
    "\n",
    "\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                loss = ((noise_pred - noise.to(device))**2).sum()\n",
    "                loss_hist.append(loss.detach().cpu().numpy()/x_batch.shape[0])\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print('epoch: {}, loss: {}'.format(e, np.array(loss_hist).mean()))\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, c, n=100):\n",
    "        x_sample = torch.randn(n, self.x_dim)\n",
    "        c_sample = c.repeat(n, 1)\n",
    "\n",
    "        for t in range(self.n_beta)[::-1]:\n",
    "            noise = torch.randn(n, self.x_dim)\n",
    "            if t==0: noise= torch.zeros(n, self.x_dim)\n",
    "\n",
    "            sigma = torch.sqrt(self.beta[t]*(1-self.alpha[t-1])/(1-self.alpha[t]))\n",
    "\n",
    "            noise_pred = self.model(x_sample.to(self.device),\n",
    "                                    c_sample.to(self.device),\n",
    "                                    torch.tensor([[t]]).float().to(self.device)).detach().cpu()\n",
    "\n",
    "            # import ipdb;ipdb.set_trace()\n",
    "            x_sample = (x_sample - self.beta[t]*noise_pred/torch.sqrt(1-self.alpha[t])) / torch.sqrt(1-self.beta[t]) + sigma * noise\n",
    "\n",
    "\n",
    "        return x_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.6292905807495117\n",
      "epoch: 1, loss: 1.8284944295883179\n",
      "epoch: 2, loss: 1.8389002084732056\n",
      "epoch: 3, loss: 1.173917531967163\n",
      "epoch: 4, loss: 1.8581647872924805\n",
      "epoch: 5, loss: 1.750816822052002\n",
      "epoch: 6, loss: 1.3658151626586914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     21\u001b[0m beta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(np\u001b[38;5;241m.\u001b[39mlinspace(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0.001\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0.9\u001b[39m), \u001b[38;5;241m300\u001b[39m))\n\u001b[0;32m     23\u001b[0m ddpm \u001b[38;5;241m=\u001b[39m ConditionalDenoisingDiffusionProbabilisticModel(\n\u001b[0;32m     24\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(yy)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[0;32m     25\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(xx)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[0;32m     26\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(beta)\u001b[38;5;241m.\u001b[39mfloat(), device, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mddpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([ddpm\u001b[38;5;241m.\u001b[39msampling(torch\u001b[38;5;241m.\u001b[39mtensor(x[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     33\u001b[0m                            \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m100\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[3], line 55\u001b[0m, in \u001b[0;36mConditionalDenoisingDiffusionProbabilisticModel.learning\u001b[1;34m(self, n_epoch)\u001b[0m\n\u001b[0;32m     51\u001b[0m         loss_hist\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m/\u001b[39mx_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     53\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, np\u001b[38;5;241m.\u001b[39marray(loss_hist)\u001b[38;5;241m.\u001b[39mmean()))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\syf26\\.conda\\envs\\dif_aug_cuda\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\syf26\\.conda\\envs\\dif_aug_cuda\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\syf26\\.conda\\envs\\dif_aug_cuda\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\syf26\\.conda\\envs\\dif_aug_cuda\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\syf26\\.conda\\envs\\dif_aug_cuda\\lib\\site-packages\\torch\\optim\\adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    363\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 364\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    367\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    N = 3000\n",
    "    x = np.linspace(0,2*np.pi, N)[:,None]\n",
    "    y1 = np.sin(x)*2\n",
    "    y2 = np.cos(x)*2\n",
    "\n",
    "\n",
    "    xx = np.concatenate([x, x], axis=0)\n",
    "    yy = np.concatenate(\n",
    "        [np.concatenate([y1+np.random.normal(0, 0.5, (N,1)), y2+np.random.normal(0, 0.1, (N,1))], axis=1),\n",
    "         np.concatenate([y2+np.random.normal(0, 0.1, (N,1)), y1+np.random.normal(0, 0.1, (N,1))], axis=1)],\n",
    "        axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    beta = np.exp(np.linspace(np.log(0.001), np.log(0.9), 300))\n",
    "    \n",
    "    ddpm = ConditionalDenoisingDiffusionProbabilisticModel(\n",
    "                torch.tensor(yy).float(),\n",
    "                torch.tensor(xx).float(),\n",
    "                torch.tensor(beta).float(), device, batch_size=32)\n",
    "\n",
    "    ddpm.learning(100)\n",
    "\n",
    "\n",
    "    print('sampling')\n",
    "    y_sample = np.concatenate([ddpm.sampling(torch.tensor(x[i:i+1]).float(), 20).numpy()\n",
    "                               for i in range(x.shape[0])[::100]])\n",
    "    x_sample = np.concatenate([np.ones(20)*xx for xx in x.ravel()[::100]])\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,projection='3d')\n",
    "    # ax.set_aspect('equal')\n",
    "    ax.plot(x, y2, y1, 'k')\n",
    "    ax.plot(x, y1, y2, 'k')\n",
    "    ax.scatter(xx, yy[:,0], yy[:,1])\n",
    "\n",
    "    ax.scatter(x_sample, y_sample[:,0], y_sample[:,1])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx shape (6000, 1)\n",
      "xx type <class 'numpy.ndarray'>\n",
      "first few values of xx [[0.        ]\n",
      " [0.00209509]\n",
      " [0.00419019]\n",
      " [0.00628528]\n",
      " [0.00838037]]\n",
      "yy shape (6000, 2)\n",
      "yy type <class 'numpy.ndarray'>\n",
      "first few values of c [[-0.40540375  2.13729504]\n",
      " [ 0.50514306  1.98670626]\n",
      " [ 0.31272015  2.02338927]\n",
      " [ 0.43812962  2.08056098]\n",
      " [ 0.10769933  1.93192631]]\n",
      "beta shape (300,)\n",
      "beta type <class 'numpy.ndarray'>\n",
      "first few values of beta [0.001      0.00102301 0.00104655 0.00107063 0.00109527]\n",
      "beta shape (300,)\n",
      "beta type <class 'numpy.ndarray'>\n",
      "first few values of beta [0.001      0.00102301 0.00104655 0.00107063 0.00109527]\n",
      "beta shape (300,)\n",
      "beta type <class 'numpy.ndarray'>\n",
      "first few values of beta [0.001      0.00102301 0.00104655 0.00107063 0.00109527]\n"
     ]
    }
   ],
   "source": [
    "print('xx shape',xx.shape)\n",
    "print('xx type', type(xx))\n",
    "print('first few values of xx', xx[:5])\n",
    "\n",
    "print('yy shape',yy.shape)\n",
    "print('yy type', type(yy))\n",
    "print('first few values of c', yy[:5])\n",
    "\n",
    "print('beta shape',beta.shape)\n",
    "print('beta type', type(beta))\n",
    "print('first few values of beta', beta[:5])\n",
    "\n",
    "print('beta shape',beta.shape)\n",
    "print('beta type', type(beta))\n",
    "print('first few values of beta', beta[:5])\n",
    "\n",
    "print('beta shape',beta.shape)\n",
    "print('beta type', type(beta))\n",
    "print('first few values of beta', beta[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Datas from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'episode_2.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# extraction: timestamp, action, position, velocity\n",
    "x = data['action'].values[:,None]\n",
    "c = data[['position', 'velocity']].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (91, 1)\n",
      "x type <class 'numpy.ndarray'>\n",
      "first few value of x [[0.99996948]\n",
      " [0.92062378]\n",
      " [0.28573608]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "c shape (91, 2)\n",
      "c type <class 'numpy.ndarray'>\n",
      "first few value of c [[-0.46770674  0.00109079]\n",
      " [-0.46565223  0.0020545 ]\n",
      " [-0.46360153  0.00205069]\n",
      " [-0.4619984   0.00160314]\n",
      " [-0.46085465  0.00114376]]\n"
     ]
    }
   ],
   "source": [
    "print('x shape',x.shape)\n",
    "print('x type', type(x))\n",
    "print('first few value of x', x[:5])\n",
    "\n",
    "print('c shape',c.shape)\n",
    "print('c type', type(c))\n",
    "print('first few value of c', c[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 156.35132740162038\n",
      "epoch: 1, loss: 360.1457609953704\n",
      "epoch: 2, loss: 2.7705617834020546\n",
      "epoch: 3, loss: 61.80036530671296\n",
      "epoch: 4, loss: 2.1579165988498263\n",
      "epoch: 5, loss: 21.58051441333912\n",
      "epoch: 6, loss: 1.59392660635489\n",
      "epoch: 7, loss: 7.072349265769676\n",
      "epoch: 8, loss: 2.205293443467882\n",
      "epoch: 9, loss: 3.011029278790509\n",
      "epoch: 10, loss: 1.7766186749493633\n",
      "epoch: 11, loss: 1.1492881774902344\n",
      "epoch: 12, loss: 1.4640203405309606\n",
      "epoch: 13, loss: 0.9820390630651403\n",
      "epoch: 14, loss: 1.2776202449092158\n",
      "epoch: 15, loss: 0.8480778446903935\n",
      "epoch: 16, loss: 1.012567025643808\n",
      "epoch: 17, loss: 0.6592386033799913\n",
      "epoch: 18, loss: 0.6930957370334201\n",
      "epoch: 19, loss: 0.7800033004195602\n",
      "epoch: 20, loss: 0.7889183892144097\n",
      "epoch: 21, loss: 0.817178867481373\n",
      "epoch: 22, loss: 0.9278875280309606\n",
      "epoch: 23, loss: 0.5183733480947988\n",
      "epoch: 24, loss: 0.8098427101417824\n",
      "epoch: 25, loss: 0.5260456932915581\n",
      "epoch: 26, loss: 0.4021931401005498\n",
      "epoch: 27, loss: 0.47934546294035735\n",
      "epoch: 28, loss: 0.31386968824598527\n",
      "epoch: 29, loss: 0.9743080139160156\n",
      "epoch: 30, loss: 0.33514079341182\n",
      "epoch: 31, loss: 0.2512192549528899\n",
      "epoch: 32, loss: 0.3817753968415437\n",
      "epoch: 33, loss: 0.42652179576732496\n",
      "epoch: 34, loss: 0.5593278672960069\n",
      "epoch: 35, loss: 0.5589727825588651\n",
      "epoch: 36, loss: 0.3338075567174841\n",
      "epoch: 37, loss: 0.2070068076804832\n",
      "epoch: 38, loss: 0.4813943792272497\n",
      "epoch: 39, loss: 0.48611859922055844\n",
      "epoch: 40, loss: 0.12675599698667173\n",
      "epoch: 41, loss: 0.3169290754530165\n",
      "epoch: 42, loss: 0.46128484937879777\n",
      "epoch: 43, loss: 0.3357393123485424\n",
      "epoch: 44, loss: 0.21180085782651548\n",
      "epoch: 45, loss: 0.15947096436112015\n",
      "epoch: 46, loss: 0.1513116094801161\n",
      "epoch: 47, loss: 0.4574758741590712\n",
      "epoch: 48, loss: 0.41966046227349174\n",
      "epoch: 49, loss: 0.512337154812283\n",
      "epoch: 50, loss: 0.5091605716281467\n",
      "epoch: 51, loss: 0.3458341669153284\n",
      "epoch: 52, loss: 0.3378033461394133\n",
      "epoch: 53, loss: 0.5575185705114294\n",
      "epoch: 54, loss: 0.4878982261375145\n",
      "epoch: 55, loss: 0.32075592323585794\n",
      "epoch: 56, loss: 0.2827130247045446\n",
      "epoch: 57, loss: 0.49530354252568\n",
      "epoch: 58, loss: 0.36185137430826825\n",
      "epoch: 59, loss: 0.3008212690000181\n",
      "epoch: 60, loss: 0.41985356366192855\n",
      "epoch: 61, loss: 0.5466708077324761\n",
      "epoch: 62, loss: 0.34806120837176285\n",
      "epoch: 63, loss: 0.36159006754557294\n",
      "epoch: 64, loss: 0.9580436282687717\n",
      "epoch: 65, loss: 0.17418562924420392\n",
      "epoch: 66, loss: 0.4262198695430049\n",
      "epoch: 67, loss: 0.25466304355197483\n",
      "epoch: 68, loss: 0.2949491606818305\n",
      "epoch: 69, loss: 0.17230455963699906\n",
      "epoch: 70, loss: 0.39577190964310255\n",
      "epoch: 71, loss: 0.39347231829607926\n",
      "epoch: 72, loss: 0.18633833637943975\n",
      "epoch: 73, loss: 0.26149287047209563\n",
      "epoch: 74, loss: 0.4025536643134223\n",
      "epoch: 75, loss: 0.47702022835060404\n",
      "epoch: 76, loss: 0.2304902606540256\n",
      "epoch: 77, loss: 0.3123231817174841\n",
      "epoch: 78, loss: 0.2377628926877622\n",
      "epoch: 79, loss: 0.3477292943883825\n",
      "epoch: 80, loss: 0.3010312186347114\n",
      "epoch: 81, loss: 0.5682408014933268\n",
      "epoch: 82, loss: 0.2912105984157986\n",
      "epoch: 83, loss: 0.3255588036996347\n",
      "epoch: 84, loss: 0.22375256926925094\n",
      "epoch: 85, loss: 0.18344734333179616\n",
      "epoch: 86, loss: 0.26133353621871386\n",
      "epoch: 87, loss: 0.2117532977351436\n",
      "epoch: 88, loss: 0.382667930037887\n",
      "epoch: 89, loss: 0.3765213577835648\n",
      "epoch: 90, loss: 0.3363138128209997\n",
      "epoch: 91, loss: 0.42903561062282985\n",
      "epoch: 92, loss: 0.5433769579286929\n",
      "epoch: 93, loss: 0.30181429121229386\n",
      "epoch: 94, loss: 0.16048351923624674\n",
      "epoch: 95, loss: 0.16442044576009116\n",
      "epoch: 96, loss: 0.33190133836534286\n",
      "epoch: 97, loss: 0.34696080949571395\n",
      "epoch: 98, loss: 0.501875983344184\n",
      "epoch: 99, loss: 0.3064146041870117\n"
     ]
    }
   ],
   "source": [
    "beta = np.exp(np.linspace(np.log(0.001), np.log(0.9), 300))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ddpm = ConditionalDenoisingDiffusionProbabilisticModel(\n",
    "                torch.tensor(x).float(),\n",
    "                torch.tensor(c).float(),\n",
    "                torch.tensor(beta).float(), device, batch_size=32)\n",
    "\n",
    "ddpm.learning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_execute(env, ddpm, device, n_steps, output_file = 'execution_log.csv'):\n",
    "    # observation from gym evironment\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    # initialize a dataframe to log the data\n",
    "    log_data =[]\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        position = observation[0]\n",
    "        velocity = observation[1]\n",
    "\n",
    "        c_tensor = torch.tensor([[position, velocity]], dtype = torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action_tensor = ddpm.sampling(c_tensor)\n",
    "        \n",
    "        action = action_tensor.cpu().numpy().flatten()\n",
    "\n",
    "        # execute action in the environment\n",
    "        observation, reward, done,_,_ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # log data into csv file\n",
    "        log_data.append({'timestamp':timestamp, 'action':action[0], 'position':position, 'velocity': velocity})\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    log_df = pd.DataFrame(log_data)\n",
    "    log_df.to_csv(output_file , index = False)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('MountainCarContinuous-v0', render_mode=\"human\")\n",
    "    ddpm = ConditionalDenoisingDiffusionProbabilisticModel(torch.tensor(x).float(),\n",
    "                torch.tensor(c).float(),\n",
    "                torch.tensor(beta).float(), device, batch_size=32)\n",
    "    predict_and_execute(env,ddpm, device, n_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif_aug_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
